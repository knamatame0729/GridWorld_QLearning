{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**! Prerequirement : Create your W&B account !**  \n",
        "https://wandb.ai/site"
      ],
      "metadata": {
        "id": "WmWTMT3GMIly"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Get the API key from your profile**  \n",
        "https://wandb.ai/authorize"
      ],
      "metadata": {
        "id": "7Aa_IMqXrNe4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb\n",
        "!wandb login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ftmpIt31Lwbb",
        "outputId": "7b0c1b41-f5bc-42b2-d020-b9cf35ccdc62"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.12/dist-packages (0.21.4)\n",
            "Requirement already satisfied: click>=8.0.1 in /usr/local/lib/python3.12/dist-packages (from wandb) (8.2.1)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (3.1.45)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from wandb) (25.0)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from wandb) (4.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (5.29.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.11.9)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.32.4)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.38.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.8 in /usr/local/lib/python3.12/dist-packages (from wandb) (4.15.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (2025.8.3)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mknamatam\u001b[0m (\u001b[33mgridworld_qlearning\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np                # needed for numerica computaion\n",
        "import matplotlib.pyplot as plt   # needed for plotting\n",
        "import wandb                      # needed for tracking metrics\n",
        "from datetime import datetime     # needed for changing project name\n",
        "\n",
        "# Project name for W&B\n",
        "project = f\"gridworld_q_learning_run_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
        "\n",
        "# Grid World setting\n",
        "ROWS, COLS = 3, 4      # number of rows and columns\n",
        "WIN_STATE = (0,3)      # goal state coordinates\n",
        "LOSE_STATE = (1,3)     # lose state coordinates\n",
        "START = (2,0)          # start state coordinates\n",
        "WALL = (1,1)           # wall state coordinates\n",
        "\n",
        "NUM_EPISODES = 500     # number of training episodes\n",
        "\n",
        "GOAL_REWARD = 1        # Reward for reachign goal\n",
        "LEARNING_RATE = 0.1    # Learning Rate\n",
        "DISCOUNT_FACTOR = 0.7  # Discount Factor\n",
        "EPSILON_DECAY = 0.999  # Epsilon decay factor\n",
        "EPSILON_RATE = 0.1     # Epsilon Rate\n",
        "\n",
        "class State:\n",
        "    \"\"\"\n",
        "    Represnets the environment state in the grid world\n",
        "    Handles position, rewards, termination, and movement\n",
        "    \"\"\"\n",
        "    def __init__(self, state=START, lose_reward=-1):\n",
        "        self.grid = np.zeros([ROWS, COLS])  # Initialize grid\n",
        "        self.state = state                  # current agent position\n",
        "        self.isEnd = False                  # flag for end of episode\n",
        "        self.lose_reward = lose_reward      # losing reward\n",
        "\n",
        "    def reward(self):\n",
        "        \"\"\"\n",
        "        Define reward\n",
        "\n",
        "        - WIN_STATE: +1\n",
        "        - LOSE_STATE: -1\n",
        "        - Other state: -0.04\n",
        "        \"\"\"\n",
        "        if self.state == WIN_STATE:\n",
        "            return GOAL_REWARD\n",
        "        elif self.state == LOSE_STATE:\n",
        "            return self.lose_reward\n",
        "        else:\n",
        "            return -0.04\n",
        "\n",
        "    def isEndFunc(self):\n",
        "        \"\"\"\n",
        "        Check if the episode ended\n",
        "        When the stat hits WIN and LOSE state, episode ends\n",
        "        \"\"\"\n",
        "        if (self.state == WIN_STATE):\n",
        "            self.isEnd = True\n",
        "\n",
        "        if (self.state == LOSE_STATE):\n",
        "            self.isEnd = True\n",
        "\n",
        "    def move(self, action):\n",
        "\n",
        "        \"\"\"\n",
        "        Define movement with stochastic outcomes\n",
        "        - 80% desired direction\n",
        "        - 10% left\n",
        "        - 10% right\n",
        "\n",
        "        Agent stay in the same cell when it hits walls/boundaries\n",
        "        \"\"\"\n",
        "\n",
        "        # deifine probabilites\n",
        "        probabilities = [0.8, 0.1, 0.1]\n",
        "\n",
        "        # sample action\n",
        "        if action == \"up\":\n",
        "            action = np.random.choice([\"up\", \"left\", \"right\"], p=probabilities)\n",
        "        elif action == \"down\":\n",
        "            action = np.random.choice([\"down\", \"left\", \"right\"], p=probabilities)\n",
        "        elif action == \"left\":\n",
        "            action = np.random.choice([\"left\", \"up\", \"down\"], p=probabilities)\n",
        "        elif action == \"right\":\n",
        "            action = np.random.choice([\"right\", \"up\", \"down\"], p=probabilities)\n",
        "\n",
        "        # compute the new position\n",
        "        i, j = self.state\n",
        "        if action == \"up\":\n",
        "            i -= 1\n",
        "        elif action == \"down\":\n",
        "            i += 1\n",
        "        elif action == \"left\":\n",
        "            j -= 1\n",
        "        elif action == \"right\":\n",
        "            j += 1\n",
        "\n",
        "        # check boundaries and wall\n",
        "        if 0 <= i < ROWS:\n",
        "            if 0 <= j < COLS:\n",
        "                if (i, j) != WALL:\n",
        "                    return (i,j)\n",
        "\n",
        "        return self.state    # stay in the same position\n",
        "\n",
        "\n",
        "class Agent:\n",
        "    \"\"\"\n",
        "    Q-Learning agent for teh grid world\n",
        "    \"\"\"\n",
        "    def __init__(self, lose_reward=-1):\n",
        "        self.actions = [\"up\", \"down\", \"left\", \"right\"]  # actions\n",
        "        self.State = State(lose_reward=lose_reward)     # initialize environment state\n",
        "        self.learning_rate = LEARNING_RATE              # learning rate\n",
        "        self.epsilon_rate = EPSILON_RATE                # exploration rate\n",
        "        self.gamma = DISCOUNT_FACTOR                    # discount factor\n",
        "        self.epsilon_decay = EPSILON_DECAY              # epsilon decay per episode\n",
        "        self.min_epsilon_rate = 0.01                    # miminum epsilon\n",
        "        self.lose_reward = lose_reward                  # reward for losing\n",
        "\n",
        "        # Initialize Q-Values: Q(s,a) for each state-action pair\n",
        "        self.Q_values = {}\n",
        "        for i in range(ROWS):\n",
        "            for j in range(COLS):\n",
        "                self.Q_values[(i,j)] = {}\n",
        "                for a in self.actions:\n",
        "                    self.Q_values[(i,j)][a] = 0.0\n",
        "\n",
        "\n",
        "        self.episode_rewards = []                       # Store rewards per episode\n",
        "        self.episode_steps = []                         # Store steps per episode\n",
        "        self.q_deltas = []                              # Store Q-Value difference\n",
        "\n",
        "\n",
        "    def get_policy(self):\n",
        "        \"\"\"\n",
        "        Return the current policy based on Q_values\n",
        "        \"\"\"\n",
        "        policy = {}\n",
        "\n",
        "        for i in range(ROWS):\n",
        "            for j in range(COLS):\n",
        "                state = (i,j)\n",
        "                if state in [WALL, WIN_STATE, LOSE_STATE]:\n",
        "                    continue\n",
        "\n",
        "                qvals = self.Q_values[state]                                             # Get Q-Values for the curent state\n",
        "                best_action = max(qvals, key=qvals.get)                                  # Find the action with the highest Q-value\n",
        "                policy[state] = best_action                                              # Add policy dictionary\n",
        "        return policy\n",
        "\n",
        "    def q_delta(self):\n",
        "        \"\"\"\n",
        "        Calculate teh average maximum difference between Q-Values and the best Q-Value\n",
        "        \"\"\"\n",
        "        q_deltas = []\n",
        "        for i in range(ROWS):\n",
        "            for j in range(COLS):\n",
        "                if (i,j) in [WALL, WIN_STATE, LOSE_STATE]:\n",
        "                    continue\n",
        "\n",
        "                qvals = self.Q_values[(i,j)]                                             # Get Q-Values for the current state\n",
        "                best_value = max(qvals.values())                                         # Find the maximum Q-Value\n",
        "                differences = [abs(value - best_value) for value in qvals.values()]      # Compute\n",
        "                max_difference = max(differences)                                        # Take the largest\n",
        "                q_deltas.append(max_difference)\n",
        "\n",
        "        # Return the mean delta\n",
        "        if q_deltas:\n",
        "            return np.mean(q_deltas)\n",
        "        else:\n",
        "            return 0.0\n",
        "\n",
        "    def chooseAction(self):\n",
        "        \"\"\"\n",
        "        Select action using epsilon-greedy:\n",
        "        - pick random feasible action with probabiliry ε\n",
        "        - pick action = arg max(Q(s, a)) with probability (1-ε)\n",
        "        \"\"\"\n",
        "        if self.State.isEnd:\n",
        "            return None\n",
        "        elif np.random.uniform(0,1) < self.epsilon_rate:\n",
        "            return np.random.choice(self.actions)\n",
        "        else:\n",
        "            qvals = self.Q_values[self.State.state]\n",
        "            return max(qvals, key=qvals.get)\n",
        "\n",
        "\n",
        "    def takeAction(self, action):\n",
        "        \"\"\"\n",
        "        Take action and return new state\n",
        "        \"\"\"\n",
        "        if action is None:\n",
        "          return self.State\n",
        "        new_position = self.State.move(action)\n",
        "        self.State.state = new_position\n",
        "        self.State.isEndFunc()\n",
        "        return self.State\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"\n",
        "        Reset environment for new episode\n",
        "        \"\"\"\n",
        "        self.State = State(lose_reward=self.lose_reward)\n",
        "\n",
        "    def train(self, num_episodes=NUM_EPISODES):\n",
        "        \"\"\"\n",
        "        Train agent using Q learning\n",
        "        Updates Q-Values and tracks episode metrics\n",
        "        Logs metrics to wandb (study)\n",
        "        \"\"\"\n",
        "        previous_policy = None\n",
        "        policy_changes = []\n",
        "\n",
        "        for i in range(num_episodes):\n",
        "            self.reset()\n",
        "            episode_reward = 0\n",
        "            steps = 0\n",
        "\n",
        "            while not self.State.isEnd:\n",
        "                # Current state and chosen action\n",
        "                s = self.State.state\n",
        "                a = self.chooseAction()\n",
        "\n",
        "                # take action, get next state and reward\n",
        "                self.State = self.takeAction(a)\n",
        "                s_next = self.State.state\n",
        "                r = self.State.reward()\n",
        "\n",
        "                episode_reward += r\n",
        "                steps += 1\n",
        "\n",
        "                # Q-learning update\n",
        "                if self.State.isEndFunc():\n",
        "                    target = r    # Terminal state (no future reward)\n",
        "                else:\n",
        "                    target = r + self.gamma * max(self.Q_values[s_next].values())\n",
        "                self.Q_values[s][a] += self.learning_rate * (target - self.Q_values[s][a])\n",
        "\n",
        "            # Record metrics\n",
        "            self.episode_rewards.append(episode_reward)\n",
        "            self.episode_steps.append(steps)\n",
        "            self.q_deltas.append(self.q_delta())\n",
        "\n",
        "            # Get current policy (best action for each state)\n",
        "            current_policy = self.get_policy()\n",
        "\n",
        "            policy_change = 0\n",
        "\n",
        "            # Compare with previous policy\n",
        "            if previous_policy is not None:\n",
        "                for state in current_policy:                                   # Iterate all states in teh curret policy\n",
        "                    if state in previous_policy:                               # Ensure the state exists in the previous policy\n",
        "                        if current_policy[state] != previous_policy[state]:    # Check if the action chaged\n",
        "                            policy_change += 1                                 # Increment\n",
        "\n",
        "            # Append\n",
        "            policy_changes.append(policy_change)\n",
        "            previous_policy = current_policy\n",
        "\n",
        "            # Log to wandb\n",
        "            wandb.log({\n",
        "                \"episode\": i,\n",
        "                f\"reward_L_{self.lose_reward}\": episode_reward,\n",
        "                f\"steps_L_{self.lose_reward}\": steps,\n",
        "                \"epsilon\": self.epsilon_rate,\n",
        "                f\"policy_change_L_{self.lose_reward}\": policy_change,\n",
        "                f\"q_delta_L_{self.lose_reward}\": self.q_deltas[-1]\n",
        "            })\n",
        "\n",
        "            # Decay exploraion rate\n",
        "            self.epsilon_rate = max(self.min_epsilon_rate, self.epsilon_rate * self.epsilon_decay)\n",
        "\n",
        "            # Progress update\n",
        "            if i % 100 == 0:\n",
        "                print(f\"Episode {i}, ε: {self.epsilon_rate:.3f}, Reward: {episode_reward:.3f}, Step: {steps}\")\n",
        "\n",
        "        print(f\"Training completed after {num_episodes} episodes.\")\n",
        "        return policy_change\n",
        "\n",
        "def plot_policy(agent):\n",
        "    # Initialize a dictonary to store\n",
        "    policy={}\n",
        "\n",
        "    for i in range(ROWS):\n",
        "        for j in range(COLS):\n",
        "            if (i,j) in [WALL, WIN_STATE, LOSE_STATE]:\n",
        "                continue\n",
        "\n",
        "            # Select the action with highest Q-Value\n",
        "            policy[(i,j)] = max(agent.Q_values[(i,j)], key=agent.Q_values[(i,j)].get)\n",
        "\n",
        "    # Set arrow dictionaly\n",
        "    arrow_dic = {\n",
        "        \"up\": \"↑\",\n",
        "        \"down\": \"↓\",\n",
        "        \"left\": \"←\",\n",
        "        \"right\": \"→\"\n",
        "    }\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(8,6))    # Create a Figure and Axes for plottig\n",
        "    ax.set_xlim(0, COLS)                     # Set the x-axis limits from 0  to number of columns\n",
        "    ax.set_ylim(0, ROWS)                     # Set the y-axis limits from 0 to number of rows\n",
        "    ax.set_xticks([])                        # Remove x-axis ticks\n",
        "    ax.set_yticks([])                        # Remoce y-axis ticks\n",
        "    ax.set_aspect('equal')                   # Set each cell square\n",
        "\n",
        "    # Draw grid lines\n",
        "    for i in range(ROWS):\n",
        "        for j in range(COLS):\n",
        "            rect = plt.Rectangle((j, ROWS-i-1), 1, 1, fill=False, edgecolor='black', linewidth=1)\n",
        "            ax.add_patch(rect)\n",
        "\n",
        "    # Add Q-Value and arrow in each cell\n",
        "    for i in range(ROWS):\n",
        "        for j in range(COLS):\n",
        "            # Convert python coordinates to matplotlib coordinates\n",
        "            y = ROWS - i - 0.5\n",
        "            x = j + 0.5\n",
        "\n",
        "            # Skip WALL, WIN, and LOSE cells\n",
        "            if (i,j) == WALL:\n",
        "                continue\n",
        "\n",
        "            if (i,j) == WIN_STATE:\n",
        "                continue\n",
        "\n",
        "            if (i,j) == LOSE_STATE:\n",
        "                continue\n",
        "\n",
        "            # Get Q-Value in (i, j)\n",
        "            qvals = agent.Q_values[(i,j)]\n",
        "\n",
        "            # Determine arrow based on policy\n",
        "            if  (i,j) in policy:\n",
        "                action = policy[(i, j)]\n",
        "                arrow = arrow_dic[action]\n",
        "            else:\n",
        "                arrow = \" \"\n",
        "\n",
        "            # Display Q-Values in four directions\n",
        "            ax.text(x, y + 0.3, f\"{qvals['up']:.2f}\", horizontalalignment='center', verticalalignment='center', fontsize=10, color='blue')\n",
        "            ax.text(x, y - 0.3, f\"{qvals['down']:.2f}\", horizontalalignment='center', verticalalignment='center', fontsize=10, color='blue')\n",
        "            ax.text(x - 0.3, y, f\"{qvals['left']:.2f}\", horizontalalignment='center', verticalalignment='center', fontsize=10, color='blue')\n",
        "            ax.text(x + 0.3, y, f\"{qvals['right']:.2f}\", horizontalalignment='center', verticalalignment='center', fontsize=10, color='blue')\n",
        "\n",
        "            # Display arrow in the center of cell\n",
        "            ax.text(x, y, arrow, horizontalalignment='center', verticalalignment='center', fontsize=16, color='black')\n",
        "\n",
        "    # Text for goal\n",
        "    gx = WIN_STATE[1] + 0.5\n",
        "    gy = ROWS - WIN_STATE[0] - 0.5\n",
        "    ax.text(gx, gy, \"G\", ha='center', va='center', fontsize=16, color=\"green\")\n",
        "\n",
        "    # Text for lose\n",
        "    lx = LOSE_STATE[1] + 0.5\n",
        "    ly = ROWS - LOSE_STATE[0] - 0.5\n",
        "    ax.text(lx, ly, \"L\", ha='center', va='center', fontsize=16, color=\"red\")\n",
        "\n",
        "    # Text for wall\n",
        "    wx = WALL[1] + 0.5\n",
        "    wy = ROWS - WALL[0] - 0.5\n",
        "    ax.text(wx, wy, \"Wall\", ha='center', va='center', fontsize=16, color=\"black\")\n",
        "\n",
        "    return fig\n",
        "\n",
        "def main():\n",
        "\n",
        "    # Hyperparameters\n",
        "    config = {\n",
        "        \"learning_rate\": LEARNING_RATE,\n",
        "        \"gamma\": DISCOUNT_FACTOR,\n",
        "        \"epsilon_decay\": EPSILON_DECAY,\n",
        "        \"epsilon_rate\": EPSILON_RATE,\n",
        "        \"episodes\": NUM_EPISODES\n",
        "    }\n",
        "    lose_rewards = [-1, -200]\n",
        "    for lose_reward in lose_rewards:\n",
        "      # Initialize W&B\n",
        "      wandb.init(project=project, config={**config, \"lose_reward\": lose_reward})\n",
        "      config = wandb.config\n",
        "      run_name = f\"learning_rate_{config.learning_rate}_gamma_{config.gamma}_epsilon_decay_{config.epsilon_decay}_epsilon_rate_{config.epsilon_rate}_lose_{config.lose_reward}\"\n",
        "      wandb.run.name = run_name\n",
        "\n",
        "\n",
        "      # Create Q-Learning agent and set hyperparameters from the sweep configration\n",
        "      agent = Agent(lose_reward=config.lose_reward)\n",
        "      agent.learning_rate = config.learning_rate\n",
        "      agent.gamma = config.gamma\n",
        "      agent.epsilon_decay = config.epsilon_decay\n",
        "      agent.epsilon_rate = config.epsilon_rate\n",
        "\n",
        "      ################################################\n",
        "      #      Train the agent (for NUM_EPISODES)      #\n",
        "      ################################################\n",
        "      agent.train(NUM_EPISODES)\n",
        "\n",
        "      ################################################\n",
        "      #          After finishiing training           #\n",
        "      ################################################\n",
        "\n",
        "      # Compute the final average reward over last 200 episodes\n",
        "      # Using last 100 rewards becasue the agent has already learned most of its policy by then\n",
        "      avg_reward = np.mean(agent.episode_rewards[-200:])\n",
        "      wandb.log({f\"final_avg_reward_L_{agent.lose_reward}\": avg_reward})\n",
        "\n",
        "      # Generate a policy map\n",
        "      fig = plot_policy(agent)\n",
        "      # Log policy map image to W&B\n",
        "      wandb.log({f\"policy_map_L_{agent.lose_reward}\": wandb.Image(fig)})\n",
        "      plt.close(fig)\n",
        "\n",
        "      # Store the policy\n",
        "      policy = {}\n",
        "\n",
        "      for i in range(ROWS):\n",
        "          for j in range(COLS):\n",
        "              state = (i,j)\n",
        "\n",
        "              # Skip wall, win, and lose state\n",
        "              if state in [WALL, WIN_STATE, LOSE_STATE]:\n",
        "                  continue\n",
        "\n",
        "              # For the current state, find the action with the highest Q-value\n",
        "              qvals = agent.Q_values[state]\n",
        "              best_action = max(qvals, key=qvals.get)\n",
        "\n",
        "              # Add it to the policy dictionary\n",
        "              policy[state] = best_action\n",
        "\n",
        "      # convert teh policy dictionary to a W&B table and log it\n",
        "      policy_table_data = []\n",
        "      for state, action in policy.items():\n",
        "          policy_table_data.append([str(state), action])\n",
        "\n",
        "      wandb.log({\n",
        "          f\"policy_table_L_{agent.lose_reward}\": wandb.Table(data=policy_table_data, columns=[\"state\", \"action\"]\n",
        "          )\n",
        "      })\n",
        "\n",
        "      wandb.finish()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "RRb1j4JfLljN",
        "outputId": "ff614887-2132-46f4-b3e5-fce643ac760d"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.21.4"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20251003_005843-n1nlp49y</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/gridworld_qlearning/gridworld_q_learning_run_20251003_005843/runs/n1nlp49y' target=\"_blank\">playful-lion-1</a></strong> to <a href='https://wandb.ai/gridworld_qlearning/gridworld_q_learning_run_20251003_005843' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/gridworld_qlearning/gridworld_q_learning_run_20251003_005843' target=\"_blank\">https://wandb.ai/gridworld_qlearning/gridworld_q_learning_run_20251003_005843</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/gridworld_qlearning/gridworld_q_learning_run_20251003_005843/runs/n1nlp49y' target=\"_blank\">https://wandb.ai/gridworld_qlearning/gridworld_q_learning_run_20251003_005843/runs/n1nlp49y</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 0, ε: 0.100, Reward: -2.120, Step: 29\n",
            "Episode 100, ε: 0.090, Reward: 0.800, Step: 6\n",
            "Episode 200, ε: 0.082, Reward: 0.640, Step: 10\n",
            "Episode 300, ε: 0.074, Reward: 0.840, Step: 5\n",
            "Episode 400, ε: 0.067, Reward: 0.760, Step: 7\n",
            "Training completed after 500 episodes.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>episode</td><td>▁▁▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇██</td></tr><tr><td>epsilon</td><td>████▇▇▇▇▇▇▆▆▆▅▅▅▅▄▄▄▄▄▄▄▄▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>final_avg_reward_L_-1</td><td>▁</td></tr><tr><td>policy_change_L_-1</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>q_delta_L_-1</td><td>▁▂▃▃▅▅▆▆▆▆▇██▇▇▇▇▇▆▆▆▆▆▆▆████▇▇▇▇▇▇▆▆▆▆▇</td></tr><tr><td>reward_L_-1</td><td>▁███▇██▇██▇▇██▇██▇█████▇██████▇██▇▇█▇█▇█</td></tr><tr><td>steps_L_-1</td><td>▂█▂▆▄▃▂▁▅▁▁▇▂▃▂▄▃▁▃▄▁▁▆▆▄▂▂▁▃▇▄▁▇▁▁▁▂▁▇▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>episode</td><td>499</td></tr><tr><td>epsilon</td><td>0.0607</td></tr><tr><td>final_avg_reward_L_-1</td><td>0.7102</td></tr><tr><td>policy_change_L_-1</td><td>0</td></tr><tr><td>q_delta_L_-1</td><td>0.25086</td></tr><tr><td>reward_L_-1</td><td>0.8</td></tr><tr><td>steps_L_-1</td><td>6</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">learning_rate_0.1_gamma_0.7_epsilon_decay_0.999_epsilon_rate_0.1_lose_-1</strong> at: <a href='https://wandb.ai/gridworld_qlearning/gridworld_q_learning_run_20251003_005843/runs/n1nlp49y' target=\"_blank\">https://wandb.ai/gridworld_qlearning/gridworld_q_learning_run_20251003_005843/runs/n1nlp49y</a><br> View project at: <a href='https://wandb.ai/gridworld_qlearning/gridworld_q_learning_run_20251003_005843' target=\"_blank\">https://wandb.ai/gridworld_qlearning/gridworld_q_learning_run_20251003_005843</a><br>Synced 5 W&B file(s), 2 media file(s), 2 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20251003_005843-n1nlp49y/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.21.4"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20251003_005845-yo4pahl4</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/gridworld_qlearning/gridworld_q_learning_run_20251003_005843/runs/yo4pahl4' target=\"_blank\">fresh-rain-2</a></strong> to <a href='https://wandb.ai/gridworld_qlearning/gridworld_q_learning_run_20251003_005843' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/gridworld_qlearning/gridworld_q_learning_run_20251003_005843' target=\"_blank\">https://wandb.ai/gridworld_qlearning/gridworld_q_learning_run_20251003_005843</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/gridworld_qlearning/gridworld_q_learning_run_20251003_005843/runs/yo4pahl4' target=\"_blank\">https://wandb.ai/gridworld_qlearning/gridworld_q_learning_run_20251003_005843/runs/yo4pahl4</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 0, ε: 0.100, Reward: -202.320, Step: 59\n",
            "Episode 100, ε: 0.090, Reward: 0.720, Step: 8\n",
            "Episode 200, ε: 0.082, Reward: 0.560, Step: 12\n",
            "Episode 300, ε: 0.074, Reward: 0.800, Step: 6\n",
            "Episode 400, ε: 0.067, Reward: 0.680, Step: 9\n",
            "Training completed after 500 episodes.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>episode</td><td>▁▁▂▂▂▂▃▃▃▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>epsilon</td><td>████▇▆▆▆▆▆▆▆▆▅▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▁▁▁</td></tr><tr><td>final_avg_reward_L_-200</td><td>▁</td></tr><tr><td>policy_change_L_-200</td><td>█▁▅▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>q_delta_L_-200</td><td>▁▂▂▂▂▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█</td></tr><tr><td>reward_L_-200</td><td>▇██▆▇▆▄████▇▇█▃█▆▅█▆█▇▇▆██████▆█▇▆▁▃▆▆█▃</td></tr><tr><td>steps_L_-200</td><td>▅▄▆▃▃▂▂▃▂▃▁▂▃▅▂▂▁▂▄▁▂▃▁▂▂▃▁▇▂▂▃█▂▁▂▃█▁▁▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>episode</td><td>499</td></tr><tr><td>epsilon</td><td>0.0607</td></tr><tr><td>final_avg_reward_L_-200</td><td>-4.3366</td></tr><tr><td>policy_change_L_-200</td><td>0</td></tr><tr><td>q_delta_L_-200</td><td>20.47882</td></tr><tr><td>reward_L_-200</td><td>0.72</td></tr><tr><td>steps_L_-200</td><td>8</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">learning_rate_0.1_gamma_0.7_epsilon_decay_0.999_epsilon_rate_0.1_lose_-200</strong> at: <a href='https://wandb.ai/gridworld_qlearning/gridworld_q_learning_run_20251003_005843/runs/yo4pahl4' target=\"_blank\">https://wandb.ai/gridworld_qlearning/gridworld_q_learning_run_20251003_005843/runs/yo4pahl4</a><br> View project at: <a href='https://wandb.ai/gridworld_qlearning/gridworld_q_learning_run_20251003_005843' target=\"_blank\">https://wandb.ai/gridworld_qlearning/gridworld_q_learning_run_20251003_005843</a><br>Synced 5 W&B file(s), 2 media file(s), 2 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20251003_005845-yo4pahl4/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "49u0WPR6MydM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}