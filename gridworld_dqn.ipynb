{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **W&B account is required to run this code**\n",
        "\n",
        "\n",
        "*   https://wandb.ai/site\n",
        "\n",
        "\n",
        "# **Get the API key from your profile**\n",
        "* https://wandb.ai/authorize\n",
        "\n",
        "\n",
        "# **Paste after this line**\n",
        "```bash\n",
        "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit: Your API key\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "fweSGJmqS1BL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "riC9KfEyOBY-",
        "outputId": "4ec46c87-82e3-4006-97a9-c474824c4549"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.12/dist-packages (0.22.2)\n",
            "Requirement already satisfied: click>=8.0.1 in /usr/local/lib/python3.12/dist-packages (from wandb) (8.3.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (3.1.45)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from wandb) (25.0)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from wandb) (4.5.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (5.29.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.11.10)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from wandb) (6.0.3)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.32.4)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.42.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.8 in /usr/local/lib/python3.12/dist-packages (from wandb) (4.15.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (2025.10.5)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mknamatam\u001b[0m (\u001b[33mgridworld_qlearning\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        }
      ],
      "source": [
        "!pip install wandb\n",
        "!wandb login"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import wandb\n",
        "from datetime import datetime\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from collections import deque\n",
        "import random\n",
        "import seaborn as sns\n",
        "\n",
        "# Log in to W&B\n",
        "wandb.login()\n",
        "\n",
        "# Project name for W&B\n",
        "project = f\"gridworld_q_learning_run_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
        "\n",
        "# ============ Grid World setting ===========\n",
        "ROWS, COLS = 3, 4      # number of rows and columns\n",
        "WIN_STATE = (0,3)      # goal state coordinates\n",
        "LOSE_STATE = (1,3)     # lose state coordinates\n",
        "START = (2,0)          # start state coordinates\n",
        "WALL = (1,1)           # wall state coordinates\n",
        "\n",
        "NUM_EPISODES = 1500     # number of training episodes\n",
        "EVAL_EPISODES = 100     # number of evaluation episodes\n",
        "\n",
        "GOAL_REWARD = 1        # Reward for reachign goal\n",
        "LOSE_REWARD = -1       # Penalty for reaching lose\n",
        "\n",
        "# ============= Hyperparameters =============\n",
        "LEARNING_RATE = 1e-3   # Learning Rate\n",
        "DISCOUNT_FACTOR = 0.99 # Discount Factor\n",
        "EPSILON_DECAY = 0.999  # Epsilon decay factor\n",
        "EPSILON_RATE = 1.0     # Epsilon Rate\n",
        "BATCH_SIZE = 128       # Mini-Batch size for replay memory\n",
        "\n",
        "\n",
        "class State:\n",
        "    \"\"\"\n",
        "    Represnets the environment state in the grid world\n",
        "    Handles position, rewards, termination, and movement\n",
        "    \"\"\"\n",
        "    def __init__(self, state=START):\n",
        "        self.grid = np.zeros([ROWS, COLS])  # Initialize grid\n",
        "        self.state = state                  # current agent position\n",
        "        self.isEnd = False                  # flag for end of episode\n",
        "\n",
        "    def reward(self):\n",
        "        \"\"\"\n",
        "        Define reward\n",
        "\n",
        "        - WIN_STATE: +1\n",
        "        - LOSE_STATE: -1\n",
        "        - Other state: -0.04\n",
        "        \"\"\"\n",
        "        if self.state == WIN_STATE:\n",
        "            return GOAL_REWARD\n",
        "        elif self.state == LOSE_STATE:\n",
        "            return LOSE_REWARD\n",
        "        else:\n",
        "            return -0.04\n",
        "\n",
        "    def isEndFunc(self):\n",
        "        \"\"\"\n",
        "        Check if the episode ended\n",
        "        When the stat hits WIN and LOSE state, episode ends\n",
        "        \"\"\"\n",
        "        if (self.state == WIN_STATE):\n",
        "            self.isEnd = True\n",
        "\n",
        "        if (self.state == LOSE_STATE):\n",
        "            self.isEnd = True\n",
        "\n",
        "    def move(self, action):\n",
        "\n",
        "        \"\"\"\n",
        "        Define movement with stochastic outcomes\n",
        "        - 80% desired direction\n",
        "        - 10% left\n",
        "        - 10% right\n",
        "\n",
        "        Agent stay in the same cell when it hits walls/boundaries\n",
        "        \"\"\"\n",
        "\n",
        "        # deifine probabilites\n",
        "        probabilities = [0.8, 0.1, 0.1]\n",
        "\n",
        "        # sample action\n",
        "        if action == \"up\":\n",
        "            action = np.random.choice([\"up\", \"left\", \"right\"], p=probabilities)\n",
        "        elif action == \"down\":\n",
        "            action = np.random.choice([\"down\", \"left\", \"right\"], p=probabilities)\n",
        "        elif action == \"left\":\n",
        "            action = np.random.choice([\"left\", \"up\", \"down\"], p=probabilities)\n",
        "        elif action == \"right\":\n",
        "            action = np.random.choice([\"right\", \"up\", \"down\"], p=probabilities)\n",
        "\n",
        "        # compute the new position\n",
        "        i, j = self.state\n",
        "        if action == \"up\":\n",
        "            i -= 1\n",
        "        elif action == \"down\":\n",
        "            i += 1\n",
        "        elif action == \"left\":\n",
        "            j -= 1\n",
        "        elif action == \"right\":\n",
        "            j += 1\n",
        "\n",
        "        # check boundaries and wall\n",
        "        if 0 <= i < ROWS:\n",
        "            if 0 <= j < COLS:\n",
        "                if (i, j) != WALL:\n",
        "                    return (i,j)\n",
        "\n",
        "        return self.state    # stay in the same position\n",
        "\n",
        "\n",
        "class DQN(nn.Module):\n",
        "    \"\"\"\n",
        "    Deep Q-Network (DQN) with 2 hidden layers and ReLU activations\n",
        "\n",
        "    - First hidden layer: input_dim -> 64 neurons, followed by ReLU activations\n",
        "    - Second hidden layer: 64 -> 64 nerons, followed by ReLU activation\n",
        "    - Output layer: 64 -> output_dim neurons (Q-values for each action)\n",
        "\n",
        "    Args:\n",
        "        input_dim (int): Dimension of input features (state space)\n",
        "        output_dim (int): Number of actions (action space)\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(DQN, self).__init__()\n",
        "        self.layer1 = nn.Linear(input_dim, 64)\n",
        "        self.layer2 = nn.Linear(64, 64)\n",
        "        self.layer3 = nn.Linear(64, output_dim)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.layer1(x))\n",
        "        x = F.relu(self.layer2(x))\n",
        "        return self.layer3(x)\n",
        "\n",
        "\n",
        "class Agent:\n",
        "    \"\"\"\n",
        "    DQN Agent\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.actions = [\"up\", \"down\", \"left\", \"right\"]  # actions\n",
        "        self.State = State()                            # initialize environment state\n",
        "        self.lr = LEARNING_RATE                         # learning rate\n",
        "        self.exp_rate = EPSILON_RATE                    # exploration rate\n",
        "        self.decay_gamma = DISCOUNT_FACTOR              # discount factor\n",
        "        self.exp_decay = EPSILON_DECAY                  # epsilon decay per episode\n",
        "        self.min_exp_rate = 0.01                        # miminum epsilon\n",
        "\n",
        "        self.memory = deque(maxlen=1000)               # replay buffer size\n",
        "        self.batch_size = BATCH_SIZE                    # batch size\n",
        "        self.target_update_freq = 10                   # target network update frequency\n",
        "\n",
        "        # Neural network initializaiton\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        if self.device.type == \"cuda\":\n",
        "          print(f\"============ GPU is available: {torch.cuda.get_device_name(0)} ===============\")\n",
        "        else:\n",
        "          print(\"============== Using CPU ===============\")\n",
        "        input_dim = ROWS * COLS\n",
        "        output_dim = len(self.actions)\n",
        "        self.policy_net = DQN(input_dim, output_dim).to(self.device)\n",
        "        self.target_net = DQN(input_dim, output_dim).to(self.device)\n",
        "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
        "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=self.lr)\n",
        "        self.loss_fn = nn.SmoothL1Loss()                                           # Huber loss for better handling outliers\n",
        "\n",
        "        self.episode_rewards = []\n",
        "        self.episode_steps = []\n",
        "        self.losses = []\n",
        "        self.episode_successes = []\n",
        "\n",
        "    def state_to_tensor(self, state):\n",
        "        \"\"\"\n",
        "        Convert the state (i, j) into a one-hot encoded tensor\n",
        "        \"\"\"\n",
        "        # Unpack the 2D state coordinates (row, column)\n",
        "        i, j = state\n",
        "\n",
        "        # Convert the 2D state position into a single flat index\n",
        "        state_flat = i * COLS + j\n",
        "\n",
        "        # Initialize a zero vector of length ROWS * COLS\n",
        "        state_one_hot = np.zeros(ROWS * COLS)\n",
        "\n",
        "        # Set the element corresponding to the current position to 1\n",
        "        state_one_hot[state_flat] = 1\n",
        "\n",
        "        # Convert the numpy array to a pytorch tensor and move it to the device (CPU/GPU)\n",
        "        return torch.FloatTensor(state_one_hot).to(self.device)\n",
        "\n",
        "    def chooseAction(self, epsilon=None):\n",
        "        \"\"\"\n",
        "        Chooses an action using an ε-greedy policy.\n",
        "        With probability ε: random action (exploration)\n",
        "        Otherwise: action with the highest Q-value (exploitation)\n",
        "        \"\"\"\n",
        "        if self.State.isEnd:\n",
        "            # If the agent hit terminate, return None\n",
        "            return None\n",
        "\n",
        "        # If epsilon value is defined, use it otherwise use exp_rate for epsilon\n",
        "        current_epsilon = epsilon if epsilon is not None else self.exp_rate\n",
        "\n",
        "        # Choose an action using the epsilon-greedy strategy\n",
        "        if np.random.uniform(0, 1) < current_epsilon:\n",
        "            # with probability epsilon, choose a random action (exploration)\n",
        "            return np.random.choice(self.actions)\n",
        "        else:\n",
        "            # Otherwise, choose the best action to teh Q-network (exploit)\n",
        "            state_tensor = self.state_to_tensor(self.State.state)\n",
        "            with torch.no_grad():\n",
        "                q_values = self.policy_net(state_tensor)\n",
        "            return self.actions[torch.argmax(q_values).item()]\n",
        "\n",
        "    def takeAction(self, action):\n",
        "        \"\"\"\n",
        "        Execute the given action and return the new state\n",
        "        \"\"\"\n",
        "        if action is None:\n",
        "            # If no action is given, return the current state\n",
        "            return self.State\n",
        "        # Compute the new position after taking the action\n",
        "        new_position = self.State.move(action)\n",
        "        # Update the internal state with the new posiion\n",
        "        self.State.state = new_position\n",
        "        # Check whether the new state is a terminal state\n",
        "        self.State.isEndFunc()\n",
        "        # Return the updated state\n",
        "        return self.State\n",
        "\n",
        "    def store_transition(self, state, action, reward, next_state, done):\n",
        "        \"\"\"\n",
        "        Store one transition (state, action, reward, next_state, done) in replay memory\n",
        "        Each trasnsition represents one step in the environment\n",
        "        \"\"\"\n",
        "        # Convert current and next states into one-hot encoded tensors\n",
        "        state_tensor = self.state_to_tensor(state)\n",
        "        next_state_tensor = self.state_to_tensor(next_state)\n",
        "        # Convert actin into index\n",
        "        action_idx = self.actions.index(action)\n",
        "        # Append the experience tuple to memory\n",
        "        self.memory.append((state_tensor, action_idx, reward, next_state_tensor, done))\n",
        "\n",
        "    def learn(self):\n",
        "        \"\"\"\n",
        "        Perform one learning step from replay memory\n",
        "        Samples a batch and updates the Q-network using the Bellman equaiton\n",
        "        \"\"\"\n",
        "        # If not enough experiences in memory, skip learning\n",
        "        if len(self.memory) < self.batch_size:\n",
        "            return None\n",
        "\n",
        "        # Randomly sample a batch of transitions (state, action, reward, next_state, done)\n",
        "        batch = random.sample(self.memory, self.batch_size)\n",
        "        states, actions, rewards, next_states, dones = zip(*batch)   # Unpack\n",
        "\n",
        "        # Convert lists of tensors to a single batched tensor\n",
        "        states = torch.stack(states)\n",
        "        actions = torch.LongTensor(actions).to(self.device)\n",
        "        rewards = torch.FloatTensor(rewards).to(self.device)\n",
        "        next_states = torch.stack(next_states)\n",
        "        dones = torch.FloatTensor(dones).to(self.device)\n",
        "\n",
        "        # Compute Q-values for current states using the policy network\n",
        "        q_values = self.policy_net(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
        "        next_q_values = self.target_net(next_states).max(1)[0].detach()\n",
        "\n",
        "        # Apply the Bellman equation\n",
        "        target_q_values = rewards + (1 - dones) * self.decay_gamma * next_q_values\n",
        "\n",
        "        # Compute the loss between predicted Q-values and target Q-values\n",
        "        loss = self.loss_fn(q_values, target_q_values)\n",
        "\n",
        "        # Perform backpropagation to update the policy network\n",
        "        self.optimizer.zero_grad()       # Reset previous gradients\n",
        "        loss.backward()                  # Compute new gradients\n",
        "        self.optimizer.step()            # Update network parameters\n",
        "\n",
        "        # Return the scalar loss value of logging\n",
        "        return loss.item()\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"\n",
        "        Reset environment for new episode\n",
        "        \"\"\"\n",
        "        self.State = State()\n",
        "\n",
        "\n",
        "    def train(self, num_episodes=NUM_EPISODES):\n",
        "        \"\"\"\n",
        "        Train the DQN agent\n",
        "        \"\"\"\n",
        "\n",
        "        # Initialize a step counter\n",
        "        step_count = 0\n",
        "\n",
        "        for i in range(num_episodes):\n",
        "\n",
        "\n",
        "            # Reset the env to the initial state\n",
        "            self.reset()\n",
        "\n",
        "            # Initialize\n",
        "            episode_reward = 0\n",
        "            steps = 0\n",
        "\n",
        "            while not self.State.isEnd:\n",
        "                # Current state and chosen action\n",
        "                s = self.State.state\n",
        "                a = self.chooseAction()\n",
        "\n",
        "                # take the chosen action, get next state and reward\n",
        "                self.State = self.takeAction(a)\n",
        "                s_next = self.State.state\n",
        "                r = self.State.reward()\n",
        "\n",
        "                # Terminal flag\n",
        "                done = 1 if self.State.isEnd else 0\n",
        "\n",
        "                episode_reward += r\n",
        "                steps += 1\n",
        "                step_count += 1\n",
        "\n",
        "                # Store the transition in replay memory\n",
        "                self.store_transition(s, a, r, s_next, done)\n",
        "\n",
        "                # Perform one learning step\n",
        "                loss = self.learn()\n",
        "\n",
        "                if loss is not None:\n",
        "                    self.losses.append(loss)\n",
        "\n",
        "                # Update target network periodically\n",
        "                if step_count % self.target_update_freq == 0:\n",
        "                    self.target_net.load_state_dict(self.policy_net.state_dict())\n",
        "\n",
        "            success = 1 if self.State.state == WIN_STATE else 0\n",
        "\n",
        "            self.episode_rewards.append(episode_reward)\n",
        "            self.episode_steps.append(steps)\n",
        "            self.episode_successes.append(success)\n",
        "\n",
        "            self.exp_rate = max(self.min_exp_rate, self.exp_rate * self.exp_decay)\n",
        "\n",
        "            # Log to wandb\n",
        "            if i >= 1300:\n",
        "                wandb.log({\n",
        "                \"episode\": i,\n",
        "                \"reward\": episode_reward,\n",
        "                \"steps\": steps,\n",
        "                \"epsilon\": self.exp_rate,\n",
        "                \"loss\": loss,\n",
        "                \"train success\": success,\n",
        "                \"avg_train_reward\": np.mean(self.episode_rewards),\n",
        "                \"avg_train_reward_200\": np.mean(self.episode_rewards[-200:])\n",
        "            })\n",
        "\n",
        "\n",
        "            # Log to wandb\n",
        "            elif loss is not None:\n",
        "                wandb.log({\n",
        "                \"episode\": i,\n",
        "                \"reward\": episode_reward,\n",
        "                \"steps\": steps,\n",
        "                \"epsilon\": self.exp_rate,\n",
        "                \"loss\": loss,\n",
        "                \"train success\": success,\n",
        "                \"avg_train_reward\": np.mean(self.episode_rewards)\n",
        "            })\n",
        "\n",
        "            else:\n",
        "                wandb.log({\n",
        "                \"episode\": i,\n",
        "                \"reward\": episode_reward,\n",
        "                \"steps\": steps,\n",
        "                \"epsilon\": self.exp_rate,\n",
        "                \"train success\": success,\n",
        "                \"avg_train_reward\": np.mean(self.episode_rewards)\n",
        "            })\n",
        "\n",
        "\n",
        "            if i % 100 == 0:\n",
        "                print(f\"Episode {i}, ε: {self.exp_rate:.3f}, Reward: {episode_reward:.3f}, Step: {steps}\")\n",
        "\n",
        "    def evaluate(self, num_eval_episodes=EVAL_EPISODES):\n",
        "        \"\"\"\n",
        "        Evaluates the trained policy\n",
        "        \"\"\"\n",
        "        # Initialize\n",
        "        eval_rewards = []\n",
        "        eval_steps = []\n",
        "        eval_successes = []\n",
        "\n",
        "        for episode in range(num_eval_episodes):\n",
        "            # Reset the env to the initial state\n",
        "            self.reset()\n",
        "\n",
        "            # Initialize reward and steps\n",
        "            episode_reward = 0\n",
        "            steps = 0\n",
        "\n",
        "            while not self.State.isEnd:\n",
        "\n",
        "                # select an action with epsilon = 0\n",
        "                a = self.chooseAction(epsilon=0.0)\n",
        "\n",
        "                # Take the selected action and transition to the next state\n",
        "                self.State = self.takeAction(a)\n",
        "\n",
        "                # Get the reward from the current state\n",
        "                r = self.State.reward()\n",
        "\n",
        "                episode_reward += r\n",
        "                steps += 1\n",
        "\n",
        "            success = 1 if self.State.state == WIN_STATE else 0\n",
        "\n",
        "            # Append\n",
        "            eval_rewards.append(episode_reward)\n",
        "            eval_steps.append(steps)\n",
        "            eval_successes.append(success)\n",
        "\n",
        "            # Log each episode separately\n",
        "            print(f\"Evaluation Episode {episode+1}: Reward = {episode_reward}, Steps = {steps}\")\n",
        "\n",
        "            wandb.log({\n",
        "                \"eval_episode\": episode + 1,\n",
        "                \"eval_reward\": episode_reward,\n",
        "                \"eval_steps\": steps,\n",
        "                \"eval_success\": success,\n",
        "                \"avg_eval_reward\": np.mean(eval_rewards)\n",
        "            })\n",
        "\n",
        "        return eval_rewards, eval_steps\n",
        "\n",
        "\n",
        "def plot_policy(agent):\n",
        "    \"\"\"\n",
        "    Visualize the learned policy\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize a dictonary to store\n",
        "    policy={}\n",
        "\n",
        "    for i in range(ROWS):\n",
        "        for j in range(COLS):\n",
        "            if (i,j) in [WALL, WIN_STATE, LOSE_STATE]:\n",
        "                continue\n",
        "\n",
        "            state_tensor = agent.state_to_tensor((i, j))\n",
        "            with torch.no_grad():\n",
        "                qvals = agent.policy_net(state_tensor).cpu().numpy()\n",
        "            policy[(i, j)] = agent.actions[np.argmax(qvals)]\n",
        "\n",
        "    # Set arrow dictionaly\n",
        "    arrow_dic = {\n",
        "        \"up\": \"↑\",\n",
        "        \"down\": \"↓\",\n",
        "        \"left\": \"←\",\n",
        "        \"right\": \"→\"\n",
        "    }\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(8,6))    # Create a Figure and Axes for plottig\n",
        "    ax.set_xlim(0, COLS)                     # Set the x-axis limits from 0  to number of columns\n",
        "    ax.set_ylim(0, ROWS)                     # Set the y-axis limits from 0 to number of rows\n",
        "    ax.set_xticks([])                        # Remove x-axis ticks\n",
        "    ax.set_yticks([])                        # Remoce y-axis ticks\n",
        "    ax.set_aspect('equal')                   # Set each cell square\n",
        "\n",
        "    # Draw grid lines\n",
        "    for i in range(ROWS):\n",
        "        for j in range(COLS):\n",
        "            rect = plt.Rectangle((j, ROWS-i-1), 1, 1, fill=False, edgecolor='black', linewidth=1)\n",
        "            ax.add_patch(rect)\n",
        "\n",
        "    # Add Q-Value and arrow in each cell\n",
        "    for i in range(ROWS):\n",
        "        for j in range(COLS):\n",
        "            # Convert python coordinates to matplotlib coordinates\n",
        "            y = ROWS - i - 0.5\n",
        "            x = j + 0.5\n",
        "\n",
        "            # Skip WALL, WIN, and LOSE cells\n",
        "            if (i,j) == WALL:\n",
        "                continue\n",
        "\n",
        "            if (i,j) == WIN_STATE:\n",
        "                continue\n",
        "\n",
        "            if (i,j) == LOSE_STATE:\n",
        "                continue\n",
        "\n",
        "            state_tensor = agent.state_to_tensor((i, j))\n",
        "            with torch.no_grad():\n",
        "                qvals = agent.policy_net(state_tensor).cpu().numpy()\n",
        "            action = policy[(i, j)]\n",
        "            arrow = arrow_dic[action]\n",
        "\n",
        "            # Display Q-Values in four directions\n",
        "            ax.text(x, y + 0.3, f\"{qvals[agent.actions.index('up')]:.2f}\", horizontalalignment='center', verticalalignment='center', fontsize=10, color='blue')\n",
        "            ax.text(x, y - 0.3, f\"{qvals[agent.actions.index('down')]:.2f}\", horizontalalignment='center', verticalalignment='center', fontsize=10, color='blue')\n",
        "            ax.text(x - 0.3, y, f\"{qvals[agent.actions.index('left')]:.2f}\", horizontalalignment='center', verticalalignment='center', fontsize=10, color='blue')\n",
        "            ax.text(x + 0.3, y, f\"{qvals[agent.actions.index('right')]:.2f}\", horizontalalignment='center', verticalalignment='center', fontsize=10, color='blue')\n",
        "\n",
        "            # Display arrow in the center of cell\n",
        "            ax.text(x, y, arrow, horizontalalignment='center', verticalalignment='center', fontsize=16, color='black')\n",
        "\n",
        "    # Text for goal\n",
        "    gx = WIN_STATE[1] + 0.5\n",
        "    gy = ROWS - WIN_STATE[0] - 0.5\n",
        "    ax.text(gx, gy, \"G\", ha='center', va='center', fontsize=16, color=\"green\")\n",
        "\n",
        "    # Text for lose\n",
        "    lx = LOSE_STATE[1] + 0.5\n",
        "    ly = ROWS - LOSE_STATE[0] - 0.5\n",
        "    ax.text(lx, ly, \"L\", ha='center', va='center', fontsize=16, color=\"red\")\n",
        "\n",
        "    # Text for wall\n",
        "    wx = WALL[1] + 0.5\n",
        "    wy = ROWS - WALL[0] - 0.5\n",
        "    ax.text(wx, wy, \"Wall\", ha='center', va='center', fontsize=16, color=\"black\")\n",
        "\n",
        "    return fig\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main training and evaluation\n",
        "    \"\"\"\n",
        "    # Hyperparameters\n",
        "    config = {\n",
        "        \"learning_rate\": LEARNING_RATE,\n",
        "        \"gamma\": DISCOUNT_FACTOR,\n",
        "        \"epsilon_decay\": EPSILON_DECAY,\n",
        "        \"epsilon_rate\": EPSILON_RATE,\n",
        "        \"episodes\": NUM_EPISODES,\n",
        "        \"lose_reward\": LOSE_REWARD\n",
        "    }\n",
        "\n",
        "    # Initialize W&B\n",
        "    wandb.init(project=project, config=config)\n",
        "    config = wandb.config\n",
        "\n",
        "    run_name = f\"learning_rate_{config.learning_rate}_gamma_{config.gamma}_epsilon_decay_{config.epsilon_decay}_epsilon_rate_{config.epsilon_rate}\"\n",
        "\n",
        "    wandb.run.name = run_name\n",
        "\n",
        "    # Create Q-Learning agent and set hyperparameters from the configration\n",
        "    agent = Agent()\n",
        "    agent.lr = config.learning_rate\n",
        "    agent.decay_gamma = config.gamma\n",
        "    agent.exp_decay = config.epsilon_decay\n",
        "    agent.exp_rate = config.epsilon_rate\n",
        "\n",
        "    # Train\n",
        "    agent.train(config.episodes)\n",
        "\n",
        "    # Evalate\n",
        "    agent.evaluate()\n",
        "\n",
        "    # Generate a policy map\n",
        "    fig = plot_policy(agent)\n",
        "    # Log policy map image to W&B\n",
        "    wandb.log({f\"policy_map\": wandb.Image(fig)})\n",
        "    plt.close(fig)\n",
        "\n",
        "    # Store the policy\n",
        "    policy = {}\n",
        "\n",
        "    for i in range(ROWS):\n",
        "        for j in range(COLS):\n",
        "            state = (i,j)\n",
        "\n",
        "            # Skip wall, win, and lose state\n",
        "            if state in [WALL, WIN_STATE, LOSE_STATE]:\n",
        "                continue\n",
        "\n",
        "            state_tensor = agent.state_to_tensor(state)\n",
        "            with torch.no_grad():\n",
        "                qvals = agent.policy_net(state_tensor)\n",
        "\n",
        "            best_action = agent.actions[torch.argmax(qvals).item()]\n",
        "\n",
        "            # Add it to the policy dictionary\n",
        "            policy[state] = best_action\n",
        "\n",
        "    wandb.finish()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "rhW2_0VNR8qi",
        "outputId": "0c99d676-7b3f-4822-8d2b-379278cca386"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n",
            "  | |_| | '_ \\/ _` / _` |  _/ -_)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mknamatam\u001b[0m (\u001b[33mgridworld_qlearning\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.22.2"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20251025_170601-igimsi7z</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/gridworld_qlearning/gridworld_q_learning_run_20251025_170601/runs/igimsi7z' target=\"_blank\">fluent-hill-1</a></strong> to <a href='https://wandb.ai/gridworld_qlearning/gridworld_q_learning_run_20251025_170601' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/gridworld_qlearning/gridworld_q_learning_run_20251025_170601' target=\"_blank\">https://wandb.ai/gridworld_qlearning/gridworld_q_learning_run_20251025_170601</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/gridworld_qlearning/gridworld_q_learning_run_20251025_170601/runs/igimsi7z' target=\"_blank\">https://wandb.ai/gridworld_qlearning/gridworld_q_learning_run_20251025_170601/runs/igimsi7z</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============ GPU is available: Tesla T4 ===============\n",
            "Episode 0, ε: 0.999, Reward: 0.320, Step: 18\n",
            "Episode 100, ε: 0.904, Reward: -1.520, Step: 14\n",
            "Episode 200, ε: 0.818, Reward: -1.160, Step: 5\n",
            "Episode 300, ε: 0.740, Reward: -1.280, Step: 8\n",
            "Episode 400, ε: 0.670, Reward: -1.480, Step: 13\n",
            "Episode 500, ε: 0.606, Reward: 0.560, Step: 12\n",
            "Episode 600, ε: 0.548, Reward: 0.280, Step: 19\n",
            "Episode 700, ε: 0.496, Reward: 0.760, Step: 7\n",
            "Episode 800, ε: 0.449, Reward: 0.800, Step: 6\n",
            "Episode 900, ε: 0.406, Reward: 0.640, Step: 10\n",
            "Episode 1000, ε: 0.367, Reward: 0.800, Step: 6\n",
            "Episode 1100, ε: 0.332, Reward: 0.800, Step: 6\n",
            "Episode 1200, ε: 0.301, Reward: 0.400, Step: 16\n",
            "Episode 1300, ε: 0.272, Reward: 0.600, Step: 11\n",
            "Episode 1400, ε: 0.246, Reward: 0.720, Step: 8\n",
            "Evaluation Episode 1: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 2: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 3: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 4: Reward = 0.6000000000000001, Steps = 11\n",
            "Evaluation Episode 5: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 6: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 7: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 8: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 9: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 10: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 11: Reward = 0.64, Steps = 10\n",
            "Evaluation Episode 12: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 13: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 14: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 15: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 16: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 17: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 18: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 19: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 20: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 21: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 22: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 23: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 24: Reward = 0.6000000000000001, Steps = 11\n",
            "Evaluation Episode 25: Reward = 0.64, Steps = 10\n",
            "Evaluation Episode 26: Reward = 0.72, Steps = 8\n",
            "Evaluation Episode 27: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 28: Reward = 0.72, Steps = 8\n",
            "Evaluation Episode 29: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 30: Reward = 0.6000000000000001, Steps = 11\n",
            "Evaluation Episode 31: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 32: Reward = -1.28, Steps = 8\n",
            "Evaluation Episode 33: Reward = 0.72, Steps = 8\n",
            "Evaluation Episode 34: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 35: Reward = 0.72, Steps = 8\n",
            "Evaluation Episode 36: Reward = 0.6000000000000001, Steps = 11\n",
            "Evaluation Episode 37: Reward = 0.6799999999999999, Steps = 9\n",
            "Evaluation Episode 38: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 39: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 40: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 41: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 42: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 43: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 44: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 45: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 46: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 47: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 48: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 49: Reward = 0.6799999999999999, Steps = 9\n",
            "Evaluation Episode 50: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 51: Reward = 0.64, Steps = 10\n",
            "Evaluation Episode 52: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 53: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 54: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 55: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 56: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 57: Reward = 0.64, Steps = 10\n",
            "Evaluation Episode 58: Reward = 0.72, Steps = 8\n",
            "Evaluation Episode 59: Reward = 0.72, Steps = 8\n",
            "Evaluation Episode 60: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 61: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 62: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 63: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 64: Reward = 0.6000000000000001, Steps = 11\n",
            "Evaluation Episode 65: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 66: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 67: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 68: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 69: Reward = 0.6000000000000001, Steps = 11\n",
            "Evaluation Episode 70: Reward = 0.6799999999999999, Steps = 9\n",
            "Evaluation Episode 71: Reward = 0.6799999999999999, Steps = 9\n",
            "Evaluation Episode 72: Reward = 0.6799999999999999, Steps = 9\n",
            "Evaluation Episode 73: Reward = 0.72, Steps = 8\n",
            "Evaluation Episode 74: Reward = 0.6799999999999999, Steps = 9\n",
            "Evaluation Episode 75: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 76: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 77: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 78: Reward = 0.72, Steps = 8\n",
            "Evaluation Episode 79: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 80: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 81: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 82: Reward = 0.72, Steps = 8\n",
            "Evaluation Episode 83: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 84: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 85: Reward = 0.72, Steps = 8\n",
            "Evaluation Episode 86: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 87: Reward = 0.64, Steps = 10\n",
            "Evaluation Episode 88: Reward = 0.72, Steps = 8\n",
            "Evaluation Episode 89: Reward = 0.52, Steps = 13\n",
            "Evaluation Episode 90: Reward = 0.72, Steps = 8\n",
            "Evaluation Episode 91: Reward = 0.72, Steps = 8\n",
            "Evaluation Episode 92: Reward = 0.4800000000000001, Steps = 14\n",
            "Evaluation Episode 93: Reward = 0.72, Steps = 8\n",
            "Evaluation Episode 94: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 95: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 96: Reward = -1.3599999999999999, Steps = 10\n",
            "Evaluation Episode 97: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 98: Reward = -1.2, Steps = 6\n",
            "Evaluation Episode 99: Reward = 0.72, Steps = 8\n",
            "Evaluation Episode 100: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 101: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 102: Reward = 0.56, Steps = 12\n",
            "Evaluation Episode 103: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 104: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 105: Reward = 0.6799999999999999, Steps = 9\n",
            "Evaluation Episode 106: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 107: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 108: Reward = 0.72, Steps = 8\n",
            "Evaluation Episode 109: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 110: Reward = 0.72, Steps = 8\n",
            "Evaluation Episode 111: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 112: Reward = 0.72, Steps = 8\n",
            "Evaluation Episode 113: Reward = 0.6799999999999999, Steps = 9\n",
            "Evaluation Episode 114: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 115: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 116: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 117: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 118: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 119: Reward = 0.6799999999999999, Steps = 9\n",
            "Evaluation Episode 120: Reward = -1.2, Steps = 6\n",
            "Evaluation Episode 121: Reward = 0.6799999999999999, Steps = 9\n",
            "Evaluation Episode 122: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 123: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 124: Reward = 0.72, Steps = 8\n",
            "Evaluation Episode 125: Reward = 0.72, Steps = 8\n",
            "Evaluation Episode 126: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 127: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 128: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 129: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 130: Reward = 0.72, Steps = 8\n",
            "Evaluation Episode 131: Reward = 0.6000000000000001, Steps = 11\n",
            "Evaluation Episode 132: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 133: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 134: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 135: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 136: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 137: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 138: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 139: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 140: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 141: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 142: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 143: Reward = 0.6799999999999999, Steps = 9\n",
            "Evaluation Episode 144: Reward = 0.72, Steps = 8\n",
            "Evaluation Episode 145: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 146: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 147: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 148: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 149: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 150: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 151: Reward = 0.72, Steps = 8\n",
            "Evaluation Episode 152: Reward = -1.2, Steps = 6\n",
            "Evaluation Episode 153: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 154: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 155: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 156: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 157: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 158: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 159: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 160: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 161: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 162: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 163: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 164: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 165: Reward = 0.6799999999999999, Steps = 9\n",
            "Evaluation Episode 166: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 167: Reward = 0.6799999999999999, Steps = 9\n",
            "Evaluation Episode 168: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 169: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 170: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 171: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 172: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 173: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 174: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 175: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 176: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 177: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 178: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 179: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 180: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 181: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 182: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 183: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 184: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 185: Reward = 0.72, Steps = 8\n",
            "Evaluation Episode 186: Reward = 0.64, Steps = 10\n",
            "Evaluation Episode 187: Reward = 0.64, Steps = 10\n",
            "Evaluation Episode 188: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 189: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 190: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 191: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 192: Reward = 0.64, Steps = 10\n",
            "Evaluation Episode 193: Reward = 0.72, Steps = 8\n",
            "Evaluation Episode 194: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 195: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 196: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 197: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 198: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 199: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 200: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 201: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 202: Reward = 0.36, Steps = 17\n",
            "Evaluation Episode 203: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 204: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 205: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 206: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 207: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 208: Reward = 0.56, Steps = 12\n",
            "Evaluation Episode 209: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 210: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 211: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 212: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 213: Reward = 0.6799999999999999, Steps = 9\n",
            "Evaluation Episode 214: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 215: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 216: Reward = 0.72, Steps = 8\n",
            "Evaluation Episode 217: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 218: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 219: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 220: Reward = 0.72, Steps = 8\n",
            "Evaluation Episode 221: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 222: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 223: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 224: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 225: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 226: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 227: Reward = 0.72, Steps = 8\n",
            "Evaluation Episode 228: Reward = 0.72, Steps = 8\n",
            "Evaluation Episode 229: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 230: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 231: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 232: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 233: Reward = 0.72, Steps = 8\n",
            "Evaluation Episode 234: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 235: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 236: Reward = 0.6799999999999999, Steps = 9\n",
            "Evaluation Episode 237: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 238: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 239: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 240: Reward = 0.64, Steps = 10\n",
            "Evaluation Episode 241: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 242: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 243: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 244: Reward = 0.72, Steps = 8\n",
            "Evaluation Episode 245: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 246: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 247: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 248: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 249: Reward = 0.6000000000000001, Steps = 11\n",
            "Evaluation Episode 250: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 251: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 252: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 253: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 254: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 255: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 256: Reward = 0.72, Steps = 8\n",
            "Evaluation Episode 257: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 258: Reward = -1.32, Steps = 9\n",
            "Evaluation Episode 259: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 260: Reward = 0.64, Steps = 10\n",
            "Evaluation Episode 261: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 262: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 263: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 264: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 265: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 266: Reward = 0.6799999999999999, Steps = 9\n",
            "Evaluation Episode 267: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 268: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 269: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 270: Reward = 0.72, Steps = 8\n",
            "Evaluation Episode 271: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 272: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 273: Reward = 0.6799999999999999, Steps = 9\n",
            "Evaluation Episode 274: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 275: Reward = 0.6799999999999999, Steps = 9\n",
            "Evaluation Episode 276: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 277: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 278: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 279: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 280: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 281: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 282: Reward = 0.64, Steps = 10\n",
            "Evaluation Episode 283: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 284: Reward = -1.28, Steps = 8\n",
            "Evaluation Episode 285: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 286: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 287: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 288: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 289: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 290: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 291: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 292: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 293: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 294: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 295: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 296: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 297: Reward = 0.6000000000000001, Steps = 11\n",
            "Evaluation Episode 298: Reward = 0.6799999999999999, Steps = 9\n",
            "Evaluation Episode 299: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 300: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 301: Reward = 0.6799999999999999, Steps = 9\n",
            "Evaluation Episode 302: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 303: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 304: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 305: Reward = 0.72, Steps = 8\n",
            "Evaluation Episode 306: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 307: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 308: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 309: Reward = 0.64, Steps = 10\n",
            "Evaluation Episode 310: Reward = 0.6000000000000001, Steps = 11\n",
            "Evaluation Episode 311: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 312: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 313: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 314: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 315: Reward = 0.56, Steps = 12\n",
            "Evaluation Episode 316: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 317: Reward = 0.6799999999999999, Steps = 9\n",
            "Evaluation Episode 318: Reward = 0.56, Steps = 12\n",
            "Evaluation Episode 319: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 320: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 321: Reward = 0.6799999999999999, Steps = 9\n",
            "Evaluation Episode 322: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 323: Reward = -1.2, Steps = 6\n",
            "Evaluation Episode 324: Reward = 0.72, Steps = 8\n",
            "Evaluation Episode 325: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 326: Reward = 0.72, Steps = 8\n",
            "Evaluation Episode 327: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 328: Reward = 0.72, Steps = 8\n",
            "Evaluation Episode 329: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 330: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 331: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 332: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 333: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 334: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 335: Reward = 0.72, Steps = 8\n",
            "Evaluation Episode 336: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 337: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 338: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 339: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 340: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 341: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 342: Reward = 0.6000000000000001, Steps = 11\n",
            "Evaluation Episode 343: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 344: Reward = 0.6799999999999999, Steps = 9\n",
            "Evaluation Episode 345: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 346: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 347: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 348: Reward = 0.72, Steps = 8\n",
            "Evaluation Episode 349: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 350: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 351: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 352: Reward = 0.72, Steps = 8\n",
            "Evaluation Episode 353: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 354: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 355: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 356: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 357: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 358: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 359: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 360: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 361: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 362: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 363: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 364: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 365: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 366: Reward = 0.44000000000000006, Steps = 15\n",
            "Evaluation Episode 367: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 368: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 369: Reward = 0.72, Steps = 8\n",
            "Evaluation Episode 370: Reward = 0.72, Steps = 8\n",
            "Evaluation Episode 371: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 372: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 373: Reward = 0.64, Steps = 10\n",
            "Evaluation Episode 374: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 375: Reward = 0.64, Steps = 10\n",
            "Evaluation Episode 376: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 377: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 378: Reward = 0.6799999999999999, Steps = 9\n",
            "Evaluation Episode 379: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 380: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 381: Reward = 0.6799999999999999, Steps = 9\n",
            "Evaluation Episode 382: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 383: Reward = 0.6799999999999999, Steps = 9\n",
            "Evaluation Episode 384: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 385: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 386: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 387: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 388: Reward = 0.6799999999999999, Steps = 9\n",
            "Evaluation Episode 389: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 390: Reward = 0.6799999999999999, Steps = 9\n",
            "Evaluation Episode 391: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 392: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 393: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 394: Reward = 0.72, Steps = 8\n",
            "Evaluation Episode 395: Reward = 0.6799999999999999, Steps = 9\n",
            "Evaluation Episode 396: Reward = 0.72, Steps = 8\n",
            "Evaluation Episode 397: Reward = 0.52, Steps = 13\n",
            "Evaluation Episode 398: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 399: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 400: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 401: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 402: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 403: Reward = 0.6799999999999999, Steps = 9\n",
            "Evaluation Episode 404: Reward = 0.64, Steps = 10\n",
            "Evaluation Episode 405: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 406: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 407: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 408: Reward = 0.6000000000000001, Steps = 11\n",
            "Evaluation Episode 409: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 410: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 411: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 412: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 413: Reward = 0.72, Steps = 8\n",
            "Evaluation Episode 414: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 415: Reward = 0.64, Steps = 10\n",
            "Evaluation Episode 416: Reward = 0.4800000000000001, Steps = 14\n",
            "Evaluation Episode 417: Reward = 0.64, Steps = 10\n",
            "Evaluation Episode 418: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 419: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 420: Reward = 0.64, Steps = 10\n",
            "Evaluation Episode 421: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 422: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 423: Reward = 0.6000000000000001, Steps = 11\n",
            "Evaluation Episode 424: Reward = 0.72, Steps = 8\n",
            "Evaluation Episode 425: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 426: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 427: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 428: Reward = 0.6799999999999999, Steps = 9\n",
            "Evaluation Episode 429: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 430: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 431: Reward = 0.72, Steps = 8\n",
            "Evaluation Episode 432: Reward = 0.72, Steps = 8\n",
            "Evaluation Episode 433: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 434: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 435: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 436: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 437: Reward = 0.6799999999999999, Steps = 9\n",
            "Evaluation Episode 438: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 439: Reward = 0.6799999999999999, Steps = 9\n",
            "Evaluation Episode 440: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 441: Reward = 0.64, Steps = 10\n",
            "Evaluation Episode 442: Reward = 0.6799999999999999, Steps = 9\n",
            "Evaluation Episode 443: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 444: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 445: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 446: Reward = 0.64, Steps = 10\n",
            "Evaluation Episode 447: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 448: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 449: Reward = 0.64, Steps = 10\n",
            "Evaluation Episode 450: Reward = 0.72, Steps = 8\n",
            "Evaluation Episode 451: Reward = -1.24, Steps = 7\n",
            "Evaluation Episode 452: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 453: Reward = 0.72, Steps = 8\n",
            "Evaluation Episode 454: Reward = 0.72, Steps = 8\n",
            "Evaluation Episode 455: Reward = 0.4800000000000001, Steps = 14\n",
            "Evaluation Episode 456: Reward = 0.72, Steps = 8\n",
            "Evaluation Episode 457: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 458: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 459: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 460: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 461: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 462: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 463: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 464: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 465: Reward = 0.72, Steps = 8\n",
            "Evaluation Episode 466: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 467: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 468: Reward = 0.72, Steps = 8\n",
            "Evaluation Episode 469: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 470: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 471: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 472: Reward = 0.72, Steps = 8\n",
            "Evaluation Episode 473: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 474: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 475: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 476: Reward = 0.72, Steps = 8\n",
            "Evaluation Episode 477: Reward = 0.72, Steps = 8\n",
            "Evaluation Episode 478: Reward = 0.6799999999999999, Steps = 9\n",
            "Evaluation Episode 479: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 480: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 481: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 482: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 483: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 484: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 485: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 486: Reward = 0.72, Steps = 8\n",
            "Evaluation Episode 487: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 488: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 489: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 490: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 491: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 492: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 493: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 494: Reward = 0.64, Steps = 10\n",
            "Evaluation Episode 495: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 496: Reward = 0.72, Steps = 8\n",
            "Evaluation Episode 497: Reward = 0.6799999999999999, Steps = 9\n",
            "Evaluation Episode 498: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 499: Reward = 0.6000000000000001, Steps = 11\n",
            "Evaluation Episode 500: Reward = 0.6799999999999999, Steps = 9\n",
            "Evaluation Episode 501: Reward = 0.6799999999999999, Steps = 9\n",
            "Evaluation Episode 502: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 503: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 504: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 505: Reward = 0.6799999999999999, Steps = 9\n",
            "Evaluation Episode 506: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 507: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 508: Reward = 0.56, Steps = 12\n",
            "Evaluation Episode 509: Reward = 0.6799999999999999, Steps = 9\n",
            "Evaluation Episode 510: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 511: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 512: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 513: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 514: Reward = 0.72, Steps = 8\n",
            "Evaluation Episode 515: Reward = 0.6799999999999999, Steps = 9\n",
            "Evaluation Episode 516: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 517: Reward = 0.52, Steps = 13\n",
            "Evaluation Episode 518: Reward = 0.72, Steps = 8\n",
            "Evaluation Episode 519: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 520: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 521: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 522: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 523: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 524: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 525: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 526: Reward = 0.72, Steps = 8\n",
            "Evaluation Episode 527: Reward = 0.72, Steps = 8\n",
            "Evaluation Episode 528: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 529: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 530: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 531: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 532: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 533: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 534: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 535: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 536: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 537: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 538: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 539: Reward = 0.72, Steps = 8\n",
            "Evaluation Episode 540: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 541: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 542: Reward = 0.6799999999999999, Steps = 9\n",
            "Evaluation Episode 543: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 544: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 545: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 546: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 547: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 548: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 549: Reward = 0.56, Steps = 12\n",
            "Evaluation Episode 550: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 551: Reward = 0.52, Steps = 13\n",
            "Evaluation Episode 552: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 553: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 554: Reward = 0.72, Steps = 8\n",
            "Evaluation Episode 555: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 556: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 557: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 558: Reward = 0.72, Steps = 8\n",
            "Evaluation Episode 559: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 560: Reward = 0.6799999999999999, Steps = 9\n",
            "Evaluation Episode 561: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 562: Reward = 0.6799999999999999, Steps = 9\n",
            "Evaluation Episode 563: Reward = 0.72, Steps = 8\n",
            "Evaluation Episode 564: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 565: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 566: Reward = 0.6799999999999999, Steps = 9\n",
            "Evaluation Episode 567: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 568: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 569: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 570: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 571: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 572: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 573: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 574: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 575: Reward = 0.6799999999999999, Steps = 9\n",
            "Evaluation Episode 576: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 577: Reward = 0.72, Steps = 8\n",
            "Evaluation Episode 578: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 579: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 580: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 581: Reward = 0.56, Steps = 12\n",
            "Evaluation Episode 582: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 583: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 584: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 585: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 586: Reward = 0.6799999999999999, Steps = 9\n",
            "Evaluation Episode 587: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 588: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 589: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 590: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 591: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 592: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 593: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 594: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 595: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 596: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 597: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 598: Reward = 0.6799999999999999, Steps = 9\n",
            "Evaluation Episode 599: Reward = 0.4800000000000001, Steps = 14\n",
            "Evaluation Episode 600: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 601: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 602: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 603: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 604: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 605: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 606: Reward = 0.72, Steps = 8\n",
            "Evaluation Episode 607: Reward = 0.72, Steps = 8\n",
            "Evaluation Episode 608: Reward = 0.4800000000000001, Steps = 14\n",
            "Evaluation Episode 609: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 610: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 611: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 612: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 613: Reward = 0.56, Steps = 12\n",
            "Evaluation Episode 614: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 615: Reward = 0.64, Steps = 10\n",
            "Evaluation Episode 616: Reward = 0.72, Steps = 8\n",
            "Evaluation Episode 617: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 618: Reward = 0.6799999999999999, Steps = 9\n",
            "Evaluation Episode 619: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 620: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 621: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 622: Reward = 0.72, Steps = 8\n",
            "Evaluation Episode 623: Reward = 0.4800000000000001, Steps = 14\n",
            "Evaluation Episode 624: Reward = 0.6799999999999999, Steps = 9\n",
            "Evaluation Episode 625: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 626: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 627: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 628: Reward = 0.72, Steps = 8\n",
            "Evaluation Episode 629: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 630: Reward = 0.52, Steps = 13\n",
            "Evaluation Episode 631: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 632: Reward = 0.6000000000000001, Steps = 11\n",
            "Evaluation Episode 633: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 634: Reward = 0.72, Steps = 8\n",
            "Evaluation Episode 635: Reward = 0.56, Steps = 12\n",
            "Evaluation Episode 636: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 637: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 638: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 639: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 640: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 641: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 642: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 643: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 644: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 645: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 646: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 647: Reward = 0.72, Steps = 8\n",
            "Evaluation Episode 648: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 649: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 650: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 651: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 652: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 653: Reward = 0.6799999999999999, Steps = 9\n",
            "Evaluation Episode 654: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 655: Reward = 0.72, Steps = 8\n",
            "Evaluation Episode 656: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 657: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 658: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 659: Reward = 0.72, Steps = 8\n",
            "Evaluation Episode 660: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 661: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 662: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 663: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 664: Reward = 0.64, Steps = 10\n",
            "Evaluation Episode 665: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 666: Reward = 0.72, Steps = 8\n",
            "Evaluation Episode 667: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 668: Reward = 0.72, Steps = 8\n",
            "Evaluation Episode 669: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 670: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 671: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 672: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 673: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 674: Reward = 0.72, Steps = 8\n",
            "Evaluation Episode 675: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 676: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 677: Reward = 0.64, Steps = 10\n",
            "Evaluation Episode 678: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 679: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 680: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 681: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 682: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 683: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 684: Reward = 0.72, Steps = 8\n",
            "Evaluation Episode 685: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 686: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 687: Reward = 0.6799999999999999, Steps = 9\n",
            "Evaluation Episode 688: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 689: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 690: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 691: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 692: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 693: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 694: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 695: Reward = 0.6799999999999999, Steps = 9\n",
            "Evaluation Episode 696: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 697: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 698: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 699: Reward = 0.64, Steps = 10\n",
            "Evaluation Episode 700: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 701: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 702: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 703: Reward = 0.56, Steps = 12\n",
            "Evaluation Episode 704: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 705: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 706: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 707: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 708: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 709: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 710: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 711: Reward = 0.72, Steps = 8\n",
            "Evaluation Episode 712: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 713: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 714: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 715: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 716: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 717: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 718: Reward = 0.72, Steps = 8\n",
            "Evaluation Episode 719: Reward = 0.6799999999999999, Steps = 9\n",
            "Evaluation Episode 720: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 721: Reward = 0.64, Steps = 10\n",
            "Evaluation Episode 722: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 723: Reward = 0.6799999999999999, Steps = 9\n",
            "Evaluation Episode 724: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 725: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 726: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 727: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 728: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 729: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 730: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 731: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 732: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 733: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 734: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 735: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 736: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 737: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 738: Reward = 0.72, Steps = 8\n",
            "Evaluation Episode 739: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 740: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 741: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 742: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 743: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 744: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 745: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 746: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 747: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 748: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 749: Reward = -1.44, Steps = 12\n",
            "Evaluation Episode 750: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 751: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 752: Reward = 0.72, Steps = 8\n",
            "Evaluation Episode 753: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 754: Reward = 0.6799999999999999, Steps = 9\n",
            "Evaluation Episode 755: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 756: Reward = 0.6799999999999999, Steps = 9\n",
            "Evaluation Episode 757: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 758: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 759: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 760: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 761: Reward = 0.72, Steps = 8\n",
            "Evaluation Episode 762: Reward = 0.72, Steps = 8\n",
            "Evaluation Episode 763: Reward = 0.72, Steps = 8\n",
            "Evaluation Episode 764: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 765: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 766: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 767: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 768: Reward = 0.72, Steps = 8\n",
            "Evaluation Episode 769: Reward = 0.56, Steps = 12\n",
            "Evaluation Episode 770: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 771: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 772: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 773: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 774: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 775: Reward = 0.72, Steps = 8\n",
            "Evaluation Episode 776: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 777: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 778: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 779: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 780: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 781: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 782: Reward = 0.44000000000000006, Steps = 15\n",
            "Evaluation Episode 783: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 784: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 785: Reward = 0.6000000000000001, Steps = 11\n",
            "Evaluation Episode 786: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 787: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 788: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 789: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 790: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 791: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 792: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 793: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 794: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 795: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 796: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 797: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 798: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 799: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 800: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 801: Reward = 0.6799999999999999, Steps = 9\n",
            "Evaluation Episode 802: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 803: Reward = 0.72, Steps = 8\n",
            "Evaluation Episode 804: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 805: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 806: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 807: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 808: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 809: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 810: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 811: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 812: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 813: Reward = 0.6000000000000001, Steps = 11\n",
            "Evaluation Episode 814: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 815: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 816: Reward = 0.72, Steps = 8\n",
            "Evaluation Episode 817: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 818: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 819: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 820: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 821: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 822: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 823: Reward = 0.72, Steps = 8\n",
            "Evaluation Episode 824: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 825: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 826: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 827: Reward = 0.6799999999999999, Steps = 9\n",
            "Evaluation Episode 828: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 829: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 830: Reward = 0.52, Steps = 13\n",
            "Evaluation Episode 831: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 832: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 833: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 834: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 835: Reward = 0.52, Steps = 13\n",
            "Evaluation Episode 836: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 837: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 838: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 839: Reward = 0.6799999999999999, Steps = 9\n",
            "Evaluation Episode 840: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 841: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 842: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 843: Reward = 0.64, Steps = 10\n",
            "Evaluation Episode 844: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 845: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 846: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 847: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 848: Reward = 0.56, Steps = 12\n",
            "Evaluation Episode 849: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 850: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 851: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 852: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 853: Reward = 0.64, Steps = 10\n",
            "Evaluation Episode 854: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 855: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 856: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 857: Reward = 0.6799999999999999, Steps = 9\n",
            "Evaluation Episode 858: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 859: Reward = 0.6799999999999999, Steps = 9\n",
            "Evaluation Episode 860: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 861: Reward = 0.6799999999999999, Steps = 9\n",
            "Evaluation Episode 862: Reward = 0.6000000000000001, Steps = 11\n",
            "Evaluation Episode 863: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 864: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 865: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 866: Reward = 0.6799999999999999, Steps = 9\n",
            "Evaluation Episode 867: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 868: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 869: Reward = 0.72, Steps = 8\n",
            "Evaluation Episode 870: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 871: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 872: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 873: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 874: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 875: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 876: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 877: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 878: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 879: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 880: Reward = 0.72, Steps = 8\n",
            "Evaluation Episode 881: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 882: Reward = 0.64, Steps = 10\n",
            "Evaluation Episode 883: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 884: Reward = 0.6799999999999999, Steps = 9\n",
            "Evaluation Episode 885: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 886: Reward = 0.72, Steps = 8\n",
            "Evaluation Episode 887: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 888: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 889: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 890: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 891: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 892: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 893: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 894: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 895: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 896: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 897: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 898: Reward = 0.72, Steps = 8\n",
            "Evaluation Episode 899: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 900: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 901: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 902: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 903: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 904: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 905: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 906: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 907: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 908: Reward = 0.6799999999999999, Steps = 9\n",
            "Evaluation Episode 909: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 910: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 911: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 912: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 913: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 914: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 915: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 916: Reward = 0.6799999999999999, Steps = 9\n",
            "Evaluation Episode 917: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 918: Reward = -1.2, Steps = 6\n",
            "Evaluation Episode 919: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 920: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 921: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 922: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 923: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 924: Reward = -1.3599999999999999, Steps = 10\n",
            "Evaluation Episode 925: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 926: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 927: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 928: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 929: Reward = 0.6799999999999999, Steps = 9\n",
            "Evaluation Episode 930: Reward = 0.72, Steps = 8\n",
            "Evaluation Episode 931: Reward = 0.6799999999999999, Steps = 9\n",
            "Evaluation Episode 932: Reward = 0.6799999999999999, Steps = 9\n",
            "Evaluation Episode 933: Reward = 0.6000000000000001, Steps = 11\n",
            "Evaluation Episode 934: Reward = 0.72, Steps = 8\n",
            "Evaluation Episode 935: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 936: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 937: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 938: Reward = 0.72, Steps = 8\n",
            "Evaluation Episode 939: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 940: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 941: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 942: Reward = 0.56, Steps = 12\n",
            "Evaluation Episode 943: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 944: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 945: Reward = 0.72, Steps = 8\n",
            "Evaluation Episode 946: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 947: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 948: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 949: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 950: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 951: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 952: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 953: Reward = 0.72, Steps = 8\n",
            "Evaluation Episode 954: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 955: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 956: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 957: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 958: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 959: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 960: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 961: Reward = 0.6799999999999999, Steps = 9\n",
            "Evaluation Episode 962: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 963: Reward = 0.6799999999999999, Steps = 9\n",
            "Evaluation Episode 964: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 965: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 966: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 967: Reward = 0.72, Steps = 8\n",
            "Evaluation Episode 968: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 969: Reward = 0.72, Steps = 8\n",
            "Evaluation Episode 970: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 971: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 972: Reward = 0.52, Steps = 13\n",
            "Evaluation Episode 973: Reward = 0.64, Steps = 10\n",
            "Evaluation Episode 974: Reward = 0.56, Steps = 12\n",
            "Evaluation Episode 975: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 976: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 977: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 978: Reward = -1.24, Steps = 7\n",
            "Evaluation Episode 979: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 980: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 981: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 982: Reward = 0.64, Steps = 10\n",
            "Evaluation Episode 983: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 984: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 985: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 986: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 987: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 988: Reward = 0.64, Steps = 10\n",
            "Evaluation Episode 989: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 990: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 991: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 992: Reward = 0.64, Steps = 10\n",
            "Evaluation Episode 993: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 994: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 995: Reward = 0.4800000000000001, Steps = 14\n",
            "Evaluation Episode 996: Reward = 0.76, Steps = 7\n",
            "Evaluation Episode 997: Reward = 0.8, Steps = 6\n",
            "Evaluation Episode 998: Reward = 0.84, Steps = 5\n",
            "Evaluation Episode 999: Reward = -1.2, Steps = 6\n",
            "Evaluation Episode 1000: Reward = 0.84, Steps = 5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_eval_reward</td><td>█▂▁▂▃▃▄▄▄▄▅▅▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▆▇▆</td></tr><tr><td>avg_train_reward</td><td>▁▃▃▄▄▅▅▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇███████████</td></tr><tr><td>avg_train_reward_200</td><td>▁▁▁▁▂▂▂▂▂▂▄▄▄▄▄▄▄▄▄▄▅▇▆▆▆▆▇▇█▇▇▇▇▇█▇██▇█</td></tr><tr><td>episode</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇██</td></tr><tr><td>epsilon</td><td>██▇▇▇▇▆▆▆▅▅▅▅▄▄▄▄▄▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>eval_episode</td><td>▁▁▁▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▆▆▇▇▇██</td></tr><tr><td>eval_reward</td><td>▇▇▆▇▆▅▄▄▇▇▇▇█▆▄█▆▂▆▇█▁█▄▇▄▇▄█▇▃▇▇██▆▅▄▇▇</td></tr><tr><td>eval_steps</td><td>▂▅▃▃▁▃▃▂▃▁▁▁▁▃▃▂▂▄█▃▄▃▃▃▁▁▆▆▃▂▂▂▁▁▃▄▅▁▅▅</td></tr><tr><td>eval_success</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss</td><td>▂▁▂▂▂▂▂▁▁▁▂█▁▃▁▃▂▂▂▁▅▅▄▁▂▁▁▂▃▁▂▁▂▁▁▃▁▁▁▁</td></tr><tr><td>+3</td><td>...</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_eval_reward</td><td>0.74276</td></tr><tr><td>avg_train_reward</td><td>0.11464</td></tr><tr><td>avg_train_reward_200</td><td>0.5958</td></tr><tr><td>episode</td><td>1499</td></tr><tr><td>epsilon</td><td>0.22296</td></tr><tr><td>eval_episode</td><td>1000</td></tr><tr><td>eval_reward</td><td>0.84</td></tr><tr><td>eval_steps</td><td>5</td></tr><tr><td>eval_success</td><td>1</td></tr><tr><td>loss</td><td>0.01061</td></tr><tr><td>+3</td><td>...</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">learning_rate_0.001_gamma_0.99_epsilon_decay_0.999_epsilon_rate_1</strong> at: <a href='https://wandb.ai/gridworld_qlearning/gridworld_q_learning_run_20251025_170601/runs/igimsi7z' target=\"_blank\">https://wandb.ai/gridworld_qlearning/gridworld_q_learning_run_20251025_170601/runs/igimsi7z</a><br> View project at: <a href='https://wandb.ai/gridworld_qlearning/gridworld_q_learning_run_20251025_170601' target=\"_blank\">https://wandb.ai/gridworld_qlearning/gridworld_q_learning_run_20251025_170601</a><br>Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20251025_170601-igimsi7z/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}